{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09381e1",
   "metadata": {},
   "source": [
    "# Auxiliary classifier using annotation\n",
    "\n",
    "## 1. Data pre-processing: \n",
    "\"train.txt\" and \"test.txt\" are the same as \"train.id\" and \"test.id\"in ./xai_data_raw_apex_reg_cls\n",
    "\n",
    "Generate: \"train_data.json\", \"test_data.json\", \"train_label_id.json\", \"test_label_id.json\"\n",
    "  \n",
    "format: dictionary  \n",
    "data:  {file_name:annotation}  \n",
    "label: {file_name:player_id}\n",
    "\n",
    "## 1.1 Using peak value from kernel density estimation as label\n",
    "Data source: \"midi_label_map_apex_reg_cls.json\"generated by running map_midi_to_label.py    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304ef234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length  846\n",
      "length 26\n",
      "719\n",
      "FINISHED!:D\n"
     ]
    }
   ],
   "source": [
    "#Data processing: dict:key:file_name, value:player annotation\n",
    "import json\n",
    "# Opening JSON file\n",
    "f = open('midi_label_map_apex_reg_cls.json')\n",
    "# returns JSON object as  a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(\"length \", len(data))\n",
    "print(\"length\", len(data[\"Schubert_D960_mv2_8bars_Score_10\"]))\n",
    "\n",
    "train_dict ={}\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))\n",
    "    for i in lines:\n",
    "        #print(i)\n",
    "        i=i.strip('\\n')\n",
    "        train_dict[i]=data[i][:-1]\n",
    "        \n",
    "        #print(train_dict[i])\n",
    "\n",
    "\n",
    "json.dump( train_dict, open( \"train_data.json\", 'w' ) )\n",
    "#f2.close()\n",
    "\n",
    "# Closing file\n",
    "f.close()\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8178e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length  846\n",
      "length 26\n",
      "Schubert_D960_mv3_8bars_9_18\n",
      "\n",
      "Schubert_D960_mv3_8bars_1_19\n",
      "\n",
      "Schubert_D960_mv3_8bars_1_11\n",
      "\n",
      "Schubert_D960_mv2_8bars_2_01\n",
      "\n",
      "Schubert_D960_mv2_8bars_10_04\n",
      "\n",
      "Schubert_D960_mv2_8bars_10_07\n",
      "\n",
      "Schubert_D960_mv2_8bars_9_12\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_17\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_34\n",
      "\n",
      "Schubert_D960_mv3_8bars_8_29\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_35\n",
      "\n",
      "Schubert_D960_mv3_8bars_1_09\n",
      "\n",
      "Schubert_D960_mv3_8bars_Score_09\n",
      "\n",
      "Schubert_D960_mv3_8bars_Score_12\n",
      "\n",
      "Schubert_D960_mv3_8bars_7_11\n",
      "\n",
      "Schubert_D960_mv3_8bars_7_37\n",
      "\n",
      "Schubert_D960_mv3_8bars_8_28\n",
      "\n",
      "Schubert_D960_mv3_8bars_7_29\n",
      "\n",
      "Schubert_D960_mv3_8bars_10_13\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_32\n",
      "\n",
      "Schubert_D960_mv3_8bars_9_08\n",
      "\n",
      "Schubert_D960_mv2_8bars_5_04\n",
      "\n",
      "Schubert_D960_mv2_8bars_1_17\n",
      "\n",
      "Schubert_D960_mv2_8bars_2_11\n",
      "\n",
      "Schubert_D960_mv2_8bars_0_03\n",
      "\n",
      "Schubert_D960_mv3_8bars_7_17\n",
      "\n",
      "Schubert_D960_mv3_8bars_12_32\n",
      "\n",
      "Schubert_D960_mv2_8bars_10_10\n",
      "\n",
      "Schubert_D960_mv3_16bars_1_12\n",
      "\n",
      "Schubert_D960_mv3_16bars_5_19\n",
      "\n",
      "Schubert_D960_mv2_8bars_12_02\n",
      "\n",
      "Schubert_D960_mv2_8bars_0_11\n",
      "\n",
      "Schubert_D960_mv2_8bars_4_16\n",
      "\n",
      "Schubert_D960_mv3_8bars_2_03\n",
      "\n",
      "Schubert_D960_mv2_16bars_8_01\n",
      "\n",
      "Schubert_D960_mv2_8bars_5_08\n",
      "\n",
      "Schubert_D960_mv3_16bars_9_07\n",
      "\n",
      "Schubert_D960_mv3_8bars_0_19\n",
      "\n",
      "Schubert_D960_mv3_8bars_3_16\n",
      "\n",
      "Schubert_D960_mv2_8bars_5_03\n",
      "\n",
      "Schubert_D960_mv2_8bars_8_12\n",
      "\n",
      "Schubert_D960_mv2_8bars_8_05\n",
      "\n",
      "Schubert_D960_mv3_8bars_1_25\n",
      "\n",
      "Schubert_D960_mv2_8bars_9_17\n",
      "\n",
      "Schubert_D960_mv3_8bars_8_04\n",
      "\n",
      "Schubert_D960_mv2_8bars_4_17\n",
      "\n",
      "Schubert_D960_mv3_8bars_8_33\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_23\n",
      "\n",
      "Schubert_D960_mv2_8bars_12_04\n",
      "\n",
      "Schubert_D960_mv3_8bars_3_15\n",
      "\n",
      "Schubert_D960_mv2_8bars_7_08\n",
      "\n",
      "Schubert_D960_mv3_8bars_Score_24\n",
      "\n",
      "Schubert_D960_mv2_8bars_8_02\n",
      "\n",
      "Schubert_D960_mv3_8bars_4_11\n",
      "\n",
      "Schubert_D960_mv2_8bars_8_16\n",
      "\n",
      "Schubert_D960_mv3_8bars_10_19\n",
      "\n",
      "Schubert_D960_mv2_8bars_0_02\n",
      "\n",
      "Schubert_D960_mv3_8bars_0_13\n",
      "\n",
      "Schubert_D960_mv3_8bars_3_35\n",
      "\n",
      "Schubert_D960_mv2_16bars_9_08\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_07\n",
      "\n",
      "Schubert_D960_mv3_16bars_9_08\n",
      "\n",
      "Schubert_D960_mv3_8bars_4_31\n",
      "\n",
      "Schubert_D960_mv2_8bars_10_17\n",
      "\n",
      "Schubert_D960_mv3_8bars_0_36\n",
      "\n",
      "Schubert_D960_mv2_8bars_5_14\n",
      "\n",
      "Schubert_D960_mv2_16bars_6_01\n",
      "\n",
      "Schubert_D960_mv3_16bars_3_12\n",
      "\n",
      "Schubert_D960_mv2_8bars_5_11\n",
      "\n",
      "Schubert_D960_mv3_8bars_9_33\n",
      "\n",
      "Schubert_D960_mv3_8bars_8_18\n",
      "\n",
      "Schubert_D960_mv3_8bars_9_01\n",
      "\n",
      "Schubert_D960_mv3_8bars_9_10\n",
      "\n",
      "Schubert_D960_mv2_8bars_7_01\n",
      "\n",
      "Schubert_D960_mv2_8bars_8_09\n",
      "\n",
      "Schubert_D960_mv3_8bars_5_02\n",
      "\n",
      "Schubert_D960_mv2_8bars_10_02\n",
      "\n",
      "Schubert_D960_mv2_8bars_3_09\n",
      "\n",
      "Schubert_D960_mv3_8bars_3_32\n",
      "\n",
      "Schubert_D960_mv2_8bars_10_16\n",
      "\n",
      "FINISHED!:D\n"
     ]
    }
   ],
   "source": [
    "#dict:key:file_name, value:player annotation\n",
    "import json\n",
    "# Opening JSON file\n",
    "f = open('midi_label_map_apex_reg_cls.json')\n",
    "# returns JSON object as  a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(\"length \", len(data))\n",
    "print(\"length\", len(data[\"Schubert_D960_mv2_8bars_Score_10\"]))\n",
    "\n",
    "test_dict ={}\n",
    "\n",
    "with open('test.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in lines:\n",
    "        print(i)\n",
    "        i=i.strip('\\n')\n",
    "        test_dict[i]=data[i][:-1]\n",
    "        \n",
    "        #print(test_dict[i])\n",
    "\n",
    "\n",
    "json.dump(test_dict, open( \"test_data.json\", 'w' ) )\n",
    "\n",
    "\n",
    "# Closing file\n",
    "f.close()\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac06ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length  719\n",
      "719\n",
      "FINISHED!:D\n"
     ]
    }
   ],
   "source": [
    "#dict:key:file_name, value:one-hot encoding player id\n",
    "import json\n",
    "# Opening JSON file\n",
    "f = open('train_data.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(\"length \", len(data))\n",
    "for file_name in data:\n",
    "    player_id = (file_name.split('/')[-1].split('.')[0].split(\"_\")[-2])\n",
    "    #print(\"file_name:\", file_name)\n",
    "    #print(\"id:\", player_id)\n",
    "    if player_id == \"Score\":\n",
    "        #print(\"SCORE\")\n",
    "        player_id = 11\n",
    "    else:\n",
    "        player_id = int(player_id)\n",
    "    #data[file_name]= [0] * 13\n",
    "    #data[file_name][player_id] = 1\n",
    "    data[file_name]= player_id\n",
    "    #print(data[file_name])\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "data_temp = json.dumps(data)\n",
    "f2 = open('train_label_id.json', 'w')\n",
    "f2.write(data_temp)\n",
    "f2.close()\n",
    "\n",
    "# Closing file\n",
    "f.close()\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbd6af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length  80\n",
      "80\n",
      "FINISHED!:D\n"
     ]
    }
   ],
   "source": [
    "#dict:key:file_name, value:one-hot encoding player id\n",
    "import json\n",
    "# Opening JSON file\n",
    "f = open('test_data.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(\"length \", len(data))\n",
    "for file_name in data:\n",
    "    player_id = (file_name.split('/')[-1].split('.')[0].split(\"_\")[-2])\n",
    "    #print(\"file_name:\", file_name)\n",
    "    #print(\"id:\", player_id)\n",
    "    if player_id == \"Score\":\n",
    "        #print(\"SCORE\")\n",
    "        player_id = 11\n",
    "    else:\n",
    "        player_id = int(player_id)\n",
    "    #data[file_name]= [0] * 13\n",
    "    #data[file_name][player_id] = 1\n",
    "    data[file_name]= player_id\n",
    "    #print(data[file_name])\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "data_temp = json.dumps(data)\n",
    "f2 = open('test_label_id.json', 'w')\n",
    "f2.write(data_temp)\n",
    "f2.close()\n",
    "\n",
    "# Closing file\n",
    "f.close()\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca841fb",
   "metadata": {},
   "source": [
    "## 1.2 Using all annotation data as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb2ded6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'dataID', 'filename', 'Question_1_1_1', 'Question_1_2_1', 'Question_1_2_2', 'Question_1_3_1', 'Question_2_1_1', 'Question_2_2_1', 'Question_3_1_1', 'Question_3_2_1', 'Question_3_3_1', 'Question_4_1_1', 'Question_4_2_1', 'Question_4_3_1', 'Question_5_1_1', 'Question_5_2_1', 'Question_5_3_1', 'Question_5_4_1', 'Question_5_5_1', 'Question_6_1_1', 'Question_6_2_1', 'Question_6_3_1', 'Question_6_4_1', 'Question_6_5_1', 'Question_7_1_1', 'Question_7_2_1', 'Question_7_3_1', 'Question_8_1_1', 'Question_8_2_1', 'Question_9_1_1', 'work_length', 'message']\n",
      "['Question_1_1_1', 'Question_1_2_1', 'Question_1_2_2', 'Question_1_3_1', 'Question_2_1_1', 'Question_2_2_1', 'Question_3_1_1', 'Question_3_2_1', 'Question_3_3_1', 'Question_4_1_1', 'Question_4_2_1', 'Question_4_3_1', 'Question_5_1_1', 'Question_5_2_1', 'Question_5_3_1', 'Question_5_4_1', 'Question_5_5_1', 'Question_6_1_1', 'Question_6_2_1', 'Question_6_3_1', 'Question_6_4_1', 'Question_6_5_1', 'Question_7_1_1', 'Question_7_2_1', 'Question_7_3_1', 'Question_8_1_1', 'Question_8_2_1', 'Question_9_1_1']\n",
      "6190\n",
      "6190\n",
      "701\n",
      "701\n",
      "Finished!:D\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "f_train = open('train.txt')\n",
    "f_train_data = f_train.readlines()\n",
    "\n",
    "f_test = open('test.txt')\n",
    "f_test_data = f_test.readlines()\n",
    "\n",
    "file = open('./total.csv', encoding=\"utf-8\")\n",
    "#csvreader = csv.reader(file)\n",
    "line_index = 0\n",
    "\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "train_label_list = []\n",
    "test_label_list = []\n",
    "# level = professional\n",
    "\n",
    "with file as myFile:  \n",
    "    lines=csv.reader(myFile)  \n",
    "    for line in lines:\n",
    "        line_index +=1\n",
    "        if line_index == 1:\n",
    "            print(line)\n",
    "            line = line[3:31]\n",
    "            print(line)\n",
    "            continue\n",
    "        #print(line)\n",
    "        file_name = line[2][:-4]\n",
    "        player_id = (file_name.split('/')[-1].split('.')[0].split(\"_\")[-2])\n",
    "        user_id = int(line[0])\n",
    "        #print(user_id)\n",
    "        #print(line[1])\n",
    "        #print(\"*\",line_index)\n",
    "        #print(\"file_name:\", file_name)\n",
    "        #print(\"id:\", player_id)\n",
    "        if player_id == \"Score\":\n",
    "            #print(\"SCORE\")\n",
    "            player_id = 11\n",
    "        else:\n",
    "            player_id = int(player_id)\n",
    "        #line = line[3:-2]\n",
    "        line = [line[3]]+line[6:31]\n",
    "        #line = line[3:31]\n",
    "        for i in range(0, len(line)):\n",
    "            if line[i] == '':\n",
    "                line[i] = 0\n",
    "            line[i] = float(line[i])\n",
    "            \n",
    "        # for training data\n",
    "  \n",
    "        if file_name+'\\n' in f_train_data:\n",
    "            #print(\"Train data!\")\n",
    "            train_data_list.append(line)\n",
    "            train_label_list.append(player_id)\n",
    "            #print(\"*\",user_id)\n",
    "        elif  file_name+'\\n' in f_test_data:\n",
    "            #print(\"Test data!\")\n",
    "            test_data_list.append(line)\n",
    "            test_label_list.append(player_id)\n",
    "            #print(\"*\",user_id)\n",
    "        #else:\n",
    "            #print(file_name)\n",
    "            #assert False\n",
    "\n",
    "print(len(train_data_list))\n",
    "print(len(train_label_list))\n",
    "print(len(test_data_list))\n",
    "print(len(test_label_list))\n",
    "\n",
    "\n",
    "\n",
    "train_data_list_temp = json.dumps(train_data_list)\n",
    "train_label_list_temp = json.dumps(train_label_list)\n",
    "test_data_list_temp = json.dumps(test_data_list)\n",
    "test_label_list_temp = json.dumps(test_label_list)\n",
    "f_train_data_list = open('cls_train_data_list_all.json', 'w')\n",
    "f_train_data_list.write(train_data_list_temp)\n",
    "f_train_data_list.close()\n",
    "\n",
    "f_train_label_list = open('cls_train_label_list_all.json', 'w')\n",
    "f_train_label_list.write(train_label_list_temp)\n",
    "f_train_label_list.close()\n",
    "\n",
    "f_test_data_list= open('cls_test_data_list_all.json', 'w')\n",
    "f_test_data_list.write(test_data_list_temp)\n",
    "f_test_data_list.close()\n",
    "\n",
    "f_test_label_list = open('cls_test_label_list_all.json', 'w')\n",
    "f_test_label_list.write(test_label_list_temp)\n",
    "f_test_label_list.close()\n",
    "\n",
    "print(\"Finished!:D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30833d19",
   "metadata": {},
   "source": [
    "## 1.3 Using annotations form professional annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b573d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'dataID', 'filename', 'Question_1_1_1', 'Question_1_2_1', 'Question_1_2_2', 'Question_1_3_1', 'Question_2_1_1', 'Question_2_2_1', 'Question_3_1_1', 'Question_3_2_1', 'Question_3_3_1', 'Question_4_1_1', 'Question_4_2_1', 'Question_4_3_1', 'Question_5_1_1', 'Question_5_2_1', 'Question_5_3_1', 'Question_5_4_1', 'Question_5_5_1', 'Question_6_1_1', 'Question_6_2_1', 'Question_6_3_1', 'Question_6_4_1', 'Question_6_5_1', 'Question_7_1_1', 'Question_7_2_1', 'Question_7_3_1', 'Question_8_1_1', 'Question_8_2_1', 'Question_9_1_1', 'work_length', 'message']\n",
      "['Question_1_1_1', 'Question_1_2_1', 'Question_1_2_2', 'Question_1_3_1', 'Question_2_1_1', 'Question_2_2_1', 'Question_3_1_1', 'Question_3_2_1', 'Question_3_3_1', 'Question_4_1_1', 'Question_4_2_1', 'Question_4_3_1', 'Question_5_1_1', 'Question_5_2_1', 'Question_5_3_1', 'Question_5_4_1', 'Question_5_5_1', 'Question_6_1_1', 'Question_6_2_1', 'Question_6_3_1', 'Question_6_4_1', 'Question_6_5_1', 'Question_7_1_1', 'Question_7_2_1', 'Question_7_3_1', 'Question_8_1_1', 'Question_8_2_1', 'Question_9_1_1']\n",
      "1901\n",
      "1901\n",
      "217\n",
      "217\n",
      "Finished!:D\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "f_train = open('train.txt')\n",
    "f_train_data = f_train.readlines()\n",
    "\n",
    "f_test = open('test.txt')\n",
    "f_test_data = f_test.readlines()\n",
    "\n",
    "file = open('./total.csv', encoding=\"utf-8\")\n",
    "#csvreader = csv.reader(file)\n",
    "line_index = 0\n",
    "\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "train_label_list = []\n",
    "test_label_list = []\n",
    "# level = professional\n",
    "annotator_id = [3,7,8,25,26,30,32,33,35,38,40,41,42,43,45,48,49,74,79,81,82,85,86,88,92,95,96,98,99,108,109,110,112,114,115,117,121]\n",
    "\n",
    "\n",
    "with file as myFile:  \n",
    "    lines=csv.reader(myFile)  \n",
    "    for line in lines:\n",
    "        line_index +=1\n",
    "        if line_index == 1:\n",
    "            print(line)\n",
    "            line = line[3:31]\n",
    "            print(line)\n",
    "            continue\n",
    "        #print(line)\n",
    "        file_name = line[2][:-4]\n",
    "        player_id = (file_name.split('/')[-1].split('.')[0].split(\"_\")[-2])\n",
    "        user_id = int(line[0])\n",
    "        #print(user_id)\n",
    "        #print(line[1])\n",
    "        #print(\"*\",line_index)\n",
    "        #print(\"file_name:\", file_name)\n",
    "        #print(\"id:\", player_id)\n",
    "        if player_id == \"Score\":\n",
    "            #print(\"SCORE\")\n",
    "            player_id = 11\n",
    "        else:\n",
    "            player_id = int(player_id)\n",
    "        #line = line[3:-2]\n",
    "        line = [line[3]]+line[6:31]\n",
    "        #line = line[3:31]\n",
    "        for i in range(0, len(line)):\n",
    "            if line[i] == '':\n",
    "                line[i] = 0\n",
    "            line[i] = float(line[i])\n",
    "            \n",
    "        # for training data\n",
    "        if user_id in annotator_id:\n",
    "            if file_name+'\\n' in f_train_data:\n",
    "                #print(\"Train data!\")\n",
    "                train_data_list.append(line)\n",
    "                train_label_list.append(player_id)\n",
    "                #print(\"*\",user_id)\n",
    "            elif  file_name+'\\n' in f_test_data:\n",
    "                #print(\"Test data!\")\n",
    "                test_data_list.append(line)\n",
    "                test_label_list.append(player_id)\n",
    "                #print(\"*\",user_id)\n",
    "            #else:\n",
    "                #print(file_name)\n",
    "                #assert False\n",
    "\n",
    "print(len(train_data_list))\n",
    "print(len(train_label_list))\n",
    "print(len(test_data_list))\n",
    "print(len(test_label_list))\n",
    "\n",
    "\n",
    "\n",
    "train_data_list_temp = json.dumps(train_data_list)\n",
    "train_label_list_temp = json.dumps(train_label_list)\n",
    "test_data_list_temp = json.dumps(test_data_list)\n",
    "test_label_list_temp = json.dumps(test_label_list)\n",
    "f_train_data_list = open('cls_train_data_list_prof.json', 'w')\n",
    "f_train_data_list.write(train_data_list_temp)\n",
    "f_train_data_list.close()\n",
    "\n",
    "f_train_label_list = open('cls_train_label_list_prof.json', 'w')\n",
    "f_train_label_list.write(train_label_list_temp)\n",
    "f_train_label_list.close()\n",
    "\n",
    "f_test_data_list= open('cls_test_data_list_prof.json', 'w')\n",
    "f_test_data_list.write(test_data_list_temp)\n",
    "f_test_data_list.close()\n",
    "\n",
    "f_test_label_list = open('cls_test_label_list_prof.json', 'w')\n",
    "f_test_label_list.write(test_label_list_temp)\n",
    "f_test_label_list.close()\n",
    "\n",
    "print(\"Finished!:D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a268fb",
   "metadata": {},
   "source": [
    "## 1.4 Using mean value as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d5f734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 846/846 [00:00<00:00, 56404.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "\n",
    "LABEL_LIST = [\"Stable beat\", \"Mechanical Tempo\", \"Intensional\", \"Regular beat change\", \"Long\", \"Cushioned\", \"Saturated (wet)\", \"Clean\", \"Subtle change\", \"Even\", \"Rich\", \"Bright\", \n",
    "\"Pure\", \"Soft\", \"Sophisticated(mellow)\", \"balanced\", \"Large range of dynamic\", \"Fast paced\", \"Flowing\", \"Swing(Flexible)\", \"Flat\", \"Harmonious\", \"Optimistic(pleasant)\", \"HIgh Energy\", \n",
    "\"Dominant(forceful)\", \"Imaginative\", \"Ethereal\", \"Convincing\"]\n",
    "LABEL_MAP = {i: label for i, label in enumerate(LABEL_LIST)}\n",
    "PIANIST_MAP = OrderedDict()\n",
    "\n",
    "file = open('total.csv', encoding=\"utf-8\")\n",
    "#file = open('total.csv', encoding=\"utf-8\")\n",
    "\n",
    "def estimate_maxima(data):\n",
    "    if len(set(data))<=1: # all datas are equal\n",
    "        return data[0]\n",
    "    kde = gaussian_kde(data)\n",
    "    no_samples = 50\n",
    "    samples = np.linspace(min(data), max(data), no_samples)\n",
    "    probs = kde.evaluate(samples)\n",
    "    #maxima_index = probs.argmax()\n",
    "    # in case if more than 1 argmaxs\n",
    "    winner = np.argwhere(probs == np.amax(probs))\n",
    "    maxima = np.average(samples[winner.flatten()])\n",
    "    return maxima\n",
    "\n",
    "# model can't predict these labels well\n",
    "# regular beat change, → remove\n",
    "# subtle change → remove\n",
    "# sophisticated,  (not sure) balanced, ~~harmonious~~ → select one\n",
    "# Dominant → related with high energy. delete\n",
    "# ethereal, imaginative, convincing → select one\n",
    "\n",
    "def midi_label_map_apex_remove_bad():\n",
    "    LABEL_TO_REMOVE = [\"Regular beat change\", \"Subtle change\", \"Sophisticated(mellow)\", \"balanced\", \"Harmonious\", \"Dominant(forceful)\", \"Imaginative\"]\n",
    "    print(len(LABEL_TO_REMOVE))\n",
    "    filtered_loc = [LABEL_LIST.index(name) for name in LABEL_TO_REMOVE]\n",
    "    filtered_loc = [loc for loc in list(range(0,28)) if loc not in filtered_loc]\n",
    "    csvreader = csv.reader(file)\n",
    "    header = []\n",
    "    header = next(csvreader)\n",
    "\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # sort by each segments\n",
    "    music_label_map = defaultdict(list)\n",
    "    for row in rows:\n",
    "        user = row[0]\n",
    "        file_name = row[2].split(\".\")[0]\n",
    "        label_row = row[3:-2]\n",
    "        # label_row = row[6:-2] \n",
    "        label_row = [label_row[loc] for loc in filtered_loc]\n",
    "        label_row = label_row[3:] # skip 1-1 ~ 1-3\n",
    "        print(len(label_row))\n",
    "        for idx, elem in enumerate(label_row):\n",
    "            if elem == \"\":\n",
    "                label_row[idx] = 0.0\n",
    "            else:\n",
    "                label_row[idx] = float(elem)\n",
    "        # skip 0\n",
    "        if 0.0 in label_row:\n",
    "            continue\n",
    "        else:\n",
    "            music_label_map[file_name].append(label_row)\n",
    "\n",
    "    music_label_map_apex = dict()\n",
    "\n",
    "    # kernel density estimation\n",
    "    for key, annot_list in tqdm(music_label_map.items()):\n",
    "        annot_list = np.array(annot_list).transpose()\n",
    "        maxima = np.array([estimate_maxima(row)/7 for row in annot_list])\n",
    "        maxima = maxima.transpose().tolist()\n",
    "        music_label_map_apex[key] = maxima\n",
    "\n",
    "    # add pianist info\n",
    "    for key, annot_list in tqdm(music_label_map_apex.items()):\n",
    "        if key.split(\"_\")[-2] not in PIANIST_MAP:\n",
    "            PIANIST_MAP[key.split(\"_\")[-2]] = len(PIANIST_MAP)\n",
    "    print(PIANIST_MAP)\n",
    "    \n",
    "    for key, annot_list in tqdm(music_label_map_apex.items()):\n",
    "        music_label_map_apex[key].append(PIANIST_MAP[key.split(\"_\")[-2]])\n",
    "\n",
    "    json.dump(music_label_map_apex, open(\"midi_label_map_apex_rm_bad.json\", 'w'))\n",
    "\n",
    "def midi_label_map_apex():\n",
    "\n",
    "    csvreader = csv.reader(file)\n",
    "    header = []\n",
    "    header = next(csvreader)\n",
    "\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # sort by each segments\n",
    "    music_label_map = defaultdict(list)\n",
    "    for row in rows:\n",
    "        user = row[0]\n",
    "        file_name = row[2].split(\".\")[0]\n",
    "        #label_row = row[3:-2]\n",
    "        label_row = [row[3]] + row[7:-2] # skip 1-2 ~ 1-3\n",
    "        for idx, elem in enumerate(label_row):\n",
    "            if elem == \"\":\n",
    "                label_row[idx] = 0.0\n",
    "            else:\n",
    "                label_row[idx] = float(elem)\n",
    "        # skip 0\n",
    "        if 0.0 in label_row:\n",
    "            continue\n",
    "        else:\n",
    "            music_label_map[file_name].append(label_row)\n",
    "\n",
    "    music_label_map_apex = dict()\n",
    "\n",
    "    # kernel density estimation\n",
    "    for key, annot_list in tqdm(music_label_map.items()):\n",
    "        annot_list = np.array(annot_list).transpose()\n",
    "        maxima = np.mean(annot_list, axis = 1)\n",
    "        maxima = maxima.tolist()\n",
    "        music_label_map_apex[key] = maxima\n",
    "\n",
    "\n",
    "\n",
    "    json.dump(music_label_map_apex, open(\"midi_label_map_apex_reg_cls_avg.json\", 'w'))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #midi_label_map_apex_filtered() \n",
    "    #midi_label_map_apex_except_filtered() \n",
    "    midi_label_map_apex() \n",
    "    #midi_label_map_apex_remove_bad() \n",
    "    #midi_label_map_stdev() \n",
    "    #midi_label_map_apex_base_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e97e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length  846\n",
      "length 25\n",
      "719\n",
      "FINISHED!:D\n"
     ]
    }
   ],
   "source": [
    "#Data processing: dict:key:file_name, value:player annotation\n",
    "import json\n",
    "# Opening JSON file\n",
    "f = open('midi_label_map_apex_reg_cls_avg.json')\n",
    "# returns JSON object as  a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(\"length \", len(data))\n",
    "print(\"length\", len(data[\"Schubert_D960_mv2_8bars_Score_10\"]))\n",
    "\n",
    "train_dict ={}\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))\n",
    "    for i in lines:\n",
    "        #print(i)\n",
    "        i=i.strip('\\n')\n",
    "        train_dict[i]=data[i]\n",
    "        \n",
    "        #print(train_dict[i])\n",
    "\n",
    "\n",
    "json.dump( train_dict, open( \"train_data_avg.json\", 'w' ) )\n",
    "#f2.close()\n",
    "\n",
    "# Closing file\n",
    "f.close()\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7451be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length  846\n",
      "length 25\n",
      "80\n",
      "FINISHED!:D\n"
     ]
    }
   ],
   "source": [
    "#Data processing: dict:key:file_name, value:player annotation\n",
    "import json\n",
    "# Opening JSON file\n",
    "f = open('midi_label_map_apex_reg_cls_avg.json')\n",
    "# returns JSON object as  a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "print(\"length \", len(data))\n",
    "print(\"length\", len(data[\"Schubert_D960_mv2_8bars_Score_10\"]))\n",
    "\n",
    "train_dict ={}\n",
    "\n",
    "with open('test.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))\n",
    "    for i in lines:\n",
    "        #print(i)\n",
    "        i=i.strip('\\n')\n",
    "        train_dict[i]=data[i]\n",
    "        \n",
    "        #print(train_dict[i])\n",
    "\n",
    "\n",
    "json.dump( train_dict, open( \"test_data_avg.json\", 'w' ) )\n",
    "#f2.close()\n",
    "\n",
    "# Closing file\n",
    "f.close()\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d82c8",
   "metadata": {},
   "source": [
    "## 2. Auxiliary_classifier:\n",
    "\n",
    "## 2.1 Using peak of kernel density estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad027f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 0.2\n",
      "RandomForest 0.175\n",
      "Adaboost 0.0875\n",
      "DecisionTree 0.15\n",
      "SGDClassifier 0.075\n",
      "MLPClassifier 0.2375\n",
      "ExtraTreesClassifier 0.1875\n",
      "RandomForestClassifier 0.125\n"
     ]
    }
   ],
   "source": [
    "#using the  kernel density estimation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "f_train_data = open('train_data.json')\n",
    "f_test_data = open('test_data.json')\n",
    "f_train_label = open('train_label_id.json')\n",
    "f_test_label = open('test_label_id.json')\n",
    "\n",
    "train_data = json.load(f_train_data)\n",
    "test_data = json.load(f_test_data)\n",
    "train_label = json.load(f_train_label)\n",
    "test_label = json.load(f_test_label)\n",
    "\n",
    "train_data_value = list(train_data.values())\n",
    "test_data_value = list(test_data.values())\n",
    "train_label_value = list(train_label.values())\n",
    "test_label_value = list(test_label.values())\n",
    "#print(test_label_value)\n",
    "\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(train_data_value,train_label_value)\n",
    "prediction = svc.predict(test_data_value)\n",
    "\n",
    "#print(\"SVC\", prediction)\n",
    "\n",
    "result= (test_label_value == prediction)\n",
    "#print(result)\n",
    "#print(result.sum()/len(result))\n",
    "print(\"SVM\",svc.score(test_data_value,test_label_value))\n",
    "\n",
    "\n",
    "clf_1 = RandomForestClassifier(n_estimators=10)\n",
    "clf_1 = clf_1.fit(train_data_value, train_label_value)\n",
    "scores = clf_1.score(test_data_value, test_label_value)\n",
    "print(\"RandomForest\",scores)\n",
    "\n",
    "\n",
    "clf_2 = AdaBoostClassifier(n_estimators=10)\n",
    "clf_2 = clf_2.fit(train_data_value, train_label_value)\n",
    "scores = clf_2.score(test_data_value, test_label_value)\n",
    "print(\"Adaboost\",scores)\n",
    "\n",
    "clf_3 = DecisionTreeClassifier()\n",
    "clf_3 = clf_3.fit(train_data_value,train_label_value)\n",
    "scores = clf_3.score(test_data_value, test_label_value)\n",
    "print(\"DecisionTree\",scores)\n",
    "\n",
    "\n",
    "\n",
    "clf_4 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=50000)\n",
    "clf_4 = clf_4.fit(train_data_value,train_label_value)\n",
    "scores = clf_4.score(test_data_value, test_label_value)\n",
    "print(\"SGDClassifier\",scores)\n",
    "\n",
    "\n",
    "clf_5 = MLPClassifier(random_state=1, max_iter=20000)\n",
    "clf_5 = clf_5.fit(train_data_value,train_label_value)\n",
    "#prob = clf_5.predict_log_proba(test_data_value)\n",
    "#print(softmax(prob))\n",
    "#prob_max = softmax(prob).argmax(axis=-1)\n",
    "#print(prob_max)\n",
    "scores = clf_5.score(test_data_value, test_label_value)\n",
    "print(\"MLPClassifier\",scores)\n",
    "\n",
    "\n",
    "clf_6 = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf_6 = clf_6.fit(train_data_value,train_label_value)\n",
    "scores = clf_6.score(test_data_value, test_label_value)\n",
    "print(\"ExtraTreesClassifier\",scores)\n",
    "\n",
    "clf_7 = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf_7 = clf_7.fit(train_data_value,train_label_value)\n",
    "scores = clf_7.score(test_data_value, test_label_value)\n",
    "print(\"RandomForestClassifier\",scores)\n",
    "\n",
    "f_train_data.close()\n",
    "f_test_data.close()\n",
    "f_train_label.close()\n",
    "f_test_label.close()\n",
    "#print(train_label_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4338249",
   "metadata": {},
   "source": [
    "## 2.2 Using annotations form professional annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1961b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901\n",
      "217\n",
      "1901\n",
      "217\n",
      "SVM 0.15668202764976957\n",
      "RandomForest 0.1336405529953917\n",
      "Adaboost 0.14746543778801843\n",
      "DecisionTree 0.1152073732718894\n",
      "SGDClassifier 0.055299539170506916\n",
      "MLPClassifier 0.15668202764976957\n",
      "ExtraTreesClassifier 0.16589861751152074\n",
      "RandomForestClassifier 0.14746543778801843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "\n",
    "f_train_data = open('cls_train_data_list.json')\n",
    "f_test_data = open('cls_test_data_list.json')\n",
    "f_train_label = open('cls_train_label_list.json')\n",
    "f_test_label = open('cls_test_label_list.json')\n",
    "\n",
    "train_data_value = json.load(f_train_data)\n",
    "test_data_value = json.load(f_test_data)\n",
    "train_label_value = json.load(f_train_label)\n",
    "test_label_value = json.load(f_test_label)\n",
    "\n",
    "#print(test_label_value)\n",
    "\n",
    "print(len(train_data_value))\n",
    "print(len(test_data_value))\n",
    "print(len(train_label_value))\n",
    "print(len(test_label_value))\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(train_data_value,train_label_value)\n",
    "prediction = svc.predict(test_data_value)\n",
    "\n",
    "#print(\"SVC\", prediction)\n",
    "\n",
    "result= (test_label_value == prediction)\n",
    "#print(result)\n",
    "#print(result.sum()/len(result))\n",
    "print(\"SVM\",svc.score(test_data_value,test_label_value))\n",
    "\n",
    "\n",
    "clf_1 = RandomForestClassifier(n_estimators=10)\n",
    "clf_1 = clf_1.fit(train_data_value, train_label_value)\n",
    "scores = clf_1.score(test_data_value, test_label_value)\n",
    "print(\"RandomForest\",scores)\n",
    "\n",
    "\n",
    "clf_2 = AdaBoostClassifier(n_estimators=10)\n",
    "clf_2 = clf_2.fit(train_data_value, train_label_value)\n",
    "scores = clf_2.score(test_data_value, test_label_value)\n",
    "print(\"Adaboost\",scores)\n",
    "\n",
    "clf_3 = DecisionTreeClassifier()\n",
    "clf_3 = clf_3.fit(train_data_value,train_label_value)\n",
    "scores = clf_3.score(test_data_value, test_label_value)\n",
    "print(\"DecisionTree\",scores)\n",
    "\n",
    "\n",
    "\n",
    "clf_4 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000)\n",
    "clf_4 = clf_4.fit(train_data_value,train_label_value)\n",
    "scores = clf_4.score(test_data_value, test_label_value)\n",
    "print(\"SGDClassifier\",scores)\n",
    "\n",
    "clf_5 = MLPClassifier(random_state=1, max_iter=20000)\n",
    "clf_5 = clf_5.fit(train_data_value,train_label_value)\n",
    "scores = clf_5.score(test_data_value, test_label_value)\n",
    "print(\"MLPClassifier\",scores)\n",
    "\n",
    "\n",
    "clf_6 = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf_6 = clf_6.fit(train_data_value,train_label_value)\n",
    "scores = clf_6.score(test_data_value, test_label_value)\n",
    "print(\"ExtraTreesClassifier\",scores)\n",
    "\n",
    "clf_7 = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf_7 = clf_7.fit(train_data_value,train_label_value)\n",
    "scores = clf_7.score(test_data_value, test_label_value)\n",
    "print(\"RandomForestClassifier\",scores)\n",
    "\n",
    "\n",
    "f_train_data.close()\n",
    "f_test_data.close()\n",
    "f_train_label.close()\n",
    "f_test_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7f887",
   "metadata": {},
   "source": [
    "## 2.3 Using all annotation data as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57909682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6190\n",
      "701\n",
      "6190\n",
      "701\n",
      "SVM 0.14550641940085593\n",
      "RandomForest 0.14550641940085593\n",
      "Adaboost 0.12553495007132667\n",
      "DecisionTree 0.1369472182596291\n",
      "SGDClassifier 0.1326676176890157\n",
      "MLPClassifier 0.12696148359486448\n",
      "ExtraTreesClassifier 0.16833095577746077\n",
      "RandomForestClassifier 0.12553495007132667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "\n",
    "f_train_data = open('cls_train_data_list_all.json')\n",
    "f_test_data = open('cls_test_data_list_all.json')\n",
    "f_train_label = open('cls_train_label_list_all.json')\n",
    "f_test_label = open('cls_test_label_list_all.json')\n",
    "\n",
    "train_data_value = json.load(f_train_data)\n",
    "test_data_value = json.load(f_test_data)\n",
    "train_label_value = json.load(f_train_label)\n",
    "test_label_value = json.load(f_test_label)\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_data_value))\n",
    "print(len(test_data_value))\n",
    "print(len(train_label_value))\n",
    "print(len(test_label_value))\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(train_data_value,train_label_value)\n",
    "prediction = svc.predict(test_data_value)\n",
    "\n",
    "#print(\"SVC\", prediction)\n",
    "\n",
    "result= (test_label_value == prediction)\n",
    "#print(result)\n",
    "#print(result.sum()/len(result))\n",
    "print(\"SVM\",svc.score(test_data_value,test_label_value))\n",
    "\n",
    "\n",
    "clf_1 = RandomForestClassifier(n_estimators=10)\n",
    "clf_1 = clf_1.fit(train_data_value, train_label_value)\n",
    "scores = clf_1.score(test_data_value, test_label_value)\n",
    "print(\"RandomForest\",scores)\n",
    "\n",
    "\n",
    "clf_2 = AdaBoostClassifier(n_estimators=10)\n",
    "clf_2 = clf_2.fit(train_data_value, train_label_value)\n",
    "scores = clf_2.score(test_data_value, test_label_value)\n",
    "print(\"Adaboost\",scores)\n",
    "\n",
    "clf_3 = DecisionTreeClassifier()\n",
    "clf_3 = clf_3.fit(train_data_value,train_label_value)\n",
    "scores = clf_3.score(test_data_value, test_label_value)\n",
    "print(\"DecisionTree\",scores)\n",
    "\n",
    "\n",
    "\n",
    "clf_4 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000)\n",
    "clf_4 = clf_4.fit(train_data_value,train_label_value)\n",
    "scores = clf_4.score(test_data_value, test_label_value)\n",
    "print(\"SGDClassifier\",scores)\n",
    "\n",
    "clf_5 = MLPClassifier(random_state=1, max_iter=20000,hidden_layer_sizes=(100,))\n",
    "clf_5 = clf_5.fit(train_data_value,train_label_value)\n",
    "scores = clf_5.score(test_data_value, test_label_value)\n",
    "print(\"MLPClassifier\",scores)\n",
    "\n",
    "\n",
    "clf_6 = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf_6 = clf_6.fit(train_data_value,train_label_value)\n",
    "scores = clf_6.score(test_data_value, test_label_value)\n",
    "print(\"ExtraTreesClassifier\",scores)\n",
    "\n",
    "clf_7 = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf_7 = clf_7.fit(train_data_value,train_label_value)\n",
    "scores = clf_7.score(test_data_value, test_label_value)\n",
    "print(\"RandomForestClassifier\",scores)\n",
    "\n",
    "\n",
    "f_train_data.close()\n",
    "f_test_data.close()\n",
    "f_train_label.close()\n",
    "f_test_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bae4fa",
   "metadata": {},
   "source": [
    "## 2.4 Using mean value as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83d8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 0.1875\n",
      "RandomForest 0.1375\n",
      "Adaboost 0.175\n",
      "DecisionTree 0.2125\n",
      "SGDClassifier 0.1125\n",
      "MLPClassifier 0.2\n",
      "ExtraTreesClassifier 0.15\n",
      "RandomForestClassifier 0.1875\n"
     ]
    }
   ],
   "source": [
    "#using the  kernel density estimation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "f_train_data = open('train_data_avg.json')\n",
    "f_test_data = open('test_data_avg.json')\n",
    "f_train_label = open('train_label_id.json')\n",
    "f_test_label = open('test_label_id.json')\n",
    "\n",
    "train_data = json.load(f_train_data)\n",
    "test_data = json.load(f_test_data)\n",
    "train_label = json.load(f_train_label)\n",
    "test_label = json.load(f_test_label)\n",
    "\n",
    "train_data_value = list(train_data.values())\n",
    "test_data_value = list(test_data.values())\n",
    "train_label_value = list(train_label.values())\n",
    "test_label_value = list(test_label.values())\n",
    "#print(test_label_value)\n",
    "\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(train_data_value,train_label_value)\n",
    "prediction = svc.predict(test_data_value)\n",
    "\n",
    "#print(\"SVC\", prediction)\n",
    "\n",
    "result= (test_label_value == prediction)\n",
    "#print(result)\n",
    "#print(result.sum()/len(result))\n",
    "print(\"SVM\",svc.score(test_data_value,test_label_value))\n",
    "\n",
    "\n",
    "clf_1 = RandomForestClassifier(n_estimators=10)\n",
    "clf_1 = clf_1.fit(train_data_value, train_label_value)\n",
    "scores = clf_1.score(test_data_value, test_label_value)\n",
    "print(\"RandomForest\",scores)\n",
    "\n",
    "\n",
    "clf_2 = AdaBoostClassifier(n_estimators=10)\n",
    "clf_2 = clf_2.fit(train_data_value, train_label_value)\n",
    "scores = clf_2.score(test_data_value, test_label_value)\n",
    "print(\"Adaboost\",scores)\n",
    "\n",
    "clf_3 = DecisionTreeClassifier()\n",
    "clf_3 = clf_3.fit(train_data_value,train_label_value)\n",
    "scores = clf_3.score(test_data_value, test_label_value)\n",
    "print(\"DecisionTree\",scores)\n",
    "\n",
    "\n",
    "\n",
    "clf_4 = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=50000)\n",
    "clf_4 = clf_4.fit(train_data_value,train_label_value)\n",
    "scores = clf_4.score(test_data_value, test_label_value)\n",
    "print(\"SGDClassifier\",scores)\n",
    "\n",
    "\n",
    "clf_5 = MLPClassifier(random_state=1, max_iter=20000)\n",
    "clf_5 = clf_5.fit(train_data_value,train_label_value)\n",
    "#prob = clf_5.predict_log_proba(test_data_value)\n",
    "#print(softmax(prob))\n",
    "#prob_max = softmax(prob).argmax(axis=-1)\n",
    "#print(prob_max)\n",
    "scores = clf_5.score(test_data_value, test_label_value)\n",
    "print(\"MLPClassifier\",scores)\n",
    "\n",
    "\n",
    "clf_6 = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf_6 = clf_6.fit(train_data_value,train_label_value)\n",
    "scores = clf_6.score(test_data_value, test_label_value)\n",
    "print(\"ExtraTreesClassifier\",scores)\n",
    "\n",
    "clf_7 = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf_7 = clf_7.fit(train_data_value,train_label_value)\n",
    "scores = clf_7.score(test_data_value, test_label_value)\n",
    "print(\"RandomForestClassifier\",scores)\n",
    "\n",
    "f_train_data.close()\n",
    "f_test_data.close()\n",
    "f_train_label.close()\n",
    "f_test_label.close()\n",
    "#print(train_label_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449cc7e",
   "metadata": {},
   "source": [
    "## 3. Ensemble learning with Weighted Average Method:\n",
    "\n",
    "The predicted probabilities of MusicBert and MLP are combined together as the final prediction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae101a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MLP\n",
    "#using the  kernel density estimation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "\n",
    "f_train_data = open('train_data.json')\n",
    "f_test_data = open('test_data.json')\n",
    "f_train_label = open('train_label_id.json')\n",
    "f_test_label = open('test_label_id.json')\n",
    "\n",
    "train_data = json.load(f_train_data)\n",
    "test_data = json.load(f_test_data)\n",
    "train_label = json.load(f_train_label)\n",
    "test_label = json.load(f_test_label)\n",
    "\n",
    "train_data_value = list(train_data.values())\n",
    "test_data_value = list(test_data.values())\n",
    "train_label_value = list(train_label.values())\n",
    "test_label_value = list(test_label.values())\n",
    "#print(test_label_value)\n",
    "\n",
    "\n",
    "clf_5 = MLPClassifier(random_state=1, max_iter=20000, hidden_layer_sizes =(100,),learning_rate_init=0.1)\n",
    "#,hidden_layer_sizes=(200,), activation='relu',learning_rate='constant', learning_rate_init=0.001,solver = 'adam'\n",
    "clf_5 = clf_5.fit(train_data_value,train_label_value)\n",
    "prob = clf_5.predict_proba(test_data_value)\n",
    "print(prob)\n",
    "prob_max = prob.argmax(axis=-1)\n",
    "print(prob_max)\n",
    "scores = clf_5.score(test_data_value, test_label_value)\n",
    "print(\"MLPClassifier\",scores)\n",
    "\n",
    "numpy.savetxt(\"MLPClassifier_test_prob.txt\", prob)\n",
    "\n",
    "content = numpy.loadtxt('MLPClassifier_test_prob.txt')\n",
    "#print(\"\\nContent in file1.txt:\\n\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "MLP_pred = np.loadtxt('MLPClassifier_test_prob.txt')\n",
    "MusicBert_pred = np.loadtxt('MusicBert_pred.txt')\n",
    "label = np.loadtxt('MusicBert_label.txt')\n",
    "#print(MLP_pred)\n",
    "'''\n",
    "T = 2\n",
    "MLP_pred = np.log(MLP_pred)\n",
    "MLP_pred= torch.from_numpy(MLP_pred)\n",
    "MLP_pred_softmax = F.softmax(MLP_pred/T,dim=-1)\n",
    "MLP_pred = MLP_pred_softmax.numpy()\n",
    "#print(MLP_pred)\n",
    "T = 0.5\n",
    "MusicBert_pred = np.log(MusicBert_pred)\n",
    "MusicBert_pred= torch.from_numpy(MusicBert_pred)\n",
    "MusicBert_pred = F.softmax(MusicBert_pred/T,dim=-1)\n",
    "MusicBert_pred = MusicBert_pred.numpy()\n",
    "'''\n",
    "\n",
    "MLP_pred_max = MLP_pred.argmax(axis=-1)\n",
    "MusicBert_pred_max = MusicBert_pred.argmax(axis=-1)\n",
    "label_index = label.argmax(axis=-1)\n",
    "\n",
    "result_MLP= (MLP_pred_max == label_index)\n",
    "result_MusicBert= (MusicBert_pred_max == label_index)\n",
    "#print(result)\n",
    "acc_MLP = result_MLP.sum()/len(result_MLP)\n",
    "acc_Musicbert = result_MusicBert.sum()/len(result_MLP)\n",
    "print(\"MLP acc\",acc_MLP)\n",
    "print(\"MusicBert acc\",acc_Musicbert)\n",
    "\n",
    "'''\n",
    "a = acc_MLP/(acc_MLP+acc_Musicbert)\n",
    "print(\"ensemble weight calculated by acc\", a)\n",
    "\n",
    "combine = a* MLP_pred+(1-a)*MusicBert_pred\n",
    "combine_max = combine.argmax(axis=-1)\n",
    "result_combine= (combine_max == label_index)\n",
    "acc = result_combine.sum()/len(result_combine)\n",
    "#print(\"weight\",a)\n",
    "print(\"Ensemble acc\",acc)\n",
    "'''\n",
    "alpha = np.linspace(0.,1,200000)\n",
    "for a in alpha:\n",
    "    combine = a* MLP_pred+(1-a)*MusicBert_pred\n",
    "    combine_max = combine.argmax(axis=-1)\n",
    "    result_combine= (combine_max == label_index)\n",
    "    acc = result_combine.sum()/len(result_combine) \n",
    "    if acc>0.8375:\n",
    "        print(acc)\n",
    "print(\"FINISHED!:D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfceb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
